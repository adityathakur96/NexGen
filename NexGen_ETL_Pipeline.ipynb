{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85291618-20c1-4df4-8504-93ce56baf352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col, regexp_replace, split, year, month, to_date\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"NexGen_ETL_Pipeline\").getOrCreate()\n",
    "\n",
    "# Define the Schema\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"date_of_purchase\", StringType(), True),  # We will convert it to Date\n",
    "    StructField(\"amount\", StringType(), True),  # We will convert ₹ to Double\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define new file paths\n",
    "input_csv_path = \"dbfs:/FileStore/tables/NexGen/final_nexgen_data.csv\"\n",
    "cleaned_output_path = \"dbfs:/FileStore/tables/NexGen_Cleaned\"\n",
    "\n",
    "#  Load CSV into a Spark DataFrame\n",
    "df = spark.read.option(\"header\", True).csv(input_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892a0dbb-357f-4dbe-8f08-c9646577c31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+-----------+----------------+-------+--------+-------------+----+-----+\n|customer_id|product_id|product_name|   category|date_of_purchase| amount|quantity|     location|year|month|\n+-----------+----------+------------+-----------+----------------+-------+--------+-------------+----+-----+\n|   CUST6597|   PROD477|     MacBook|Electronics|      2024-10-01|1581.29|    2000|San Francisco|2024|   10|\n|   CUST9605|   PROD746|     MacBook|Electronics|      2024-10-30| 534.75|    4000|       Dallas|2024|   10|\n|   CUST9329|   PROD278|      iPhone|Electronics|      2024-04-16|1358.19|    1000|      Chicago|2024|    4|\n|   CUST6601|   PROD669|  Galaxy Tab|  Computers|      2024-12-20|1670.74|    5000|      Seattle|2024|   12|\n|   CUST6616|   PROD392|        iPad|Electronics|      2024-04-15| 953.72|    1000|      Seattle|2024|    4|\n+-----------+----------+------------+-----------+----------------+-------+--------+-------------+----+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Define the correct schema and clean the data\n",
    "df_cleaned = df \\\n",
    "    .withColumn(\"amount\", regexp_replace(col(\"amount\"), \"₹\", \"\")) \\\n",
    "    .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "    .withColumn(\"customer_id\", split(col(\"customer_id\"), \",\").getItem(0)) \\\n",
    "    .withColumn(\"year\", year(col(\"date_of_purchase\")).cast(\"integer\")) \\\n",
    "    .withColumn(\"month\", month(col(\"date_of_purchase\")).cast(\"integer\")) \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"integer\")) \n",
    "\n",
    "# Show cleaned data preview\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c695b7-236a-4a98-8083-d0fcc4afdf22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved at: dbfs:/FileStore/tables/NexGen_Cleaned\n"
     ]
    }
   ],
   "source": [
    "# Save as CSV in a new clean location\n",
    "df_cleaned.write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(cleaned_output_path)\n",
    "\n",
    "print(\"Cleaned CSV saved at:\", cleaned_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e85aa18-b7f7-4b40-83c1-7d0e48f8874b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: [FileInfo(path='dbfs:/FileStore/tables/NexGen_Cleaned/_committed_1762984584231884728', name='_committed_1762984584231884728', size=212, modificationTime=1739624165000),\n FileInfo(path='dbfs:/FileStore/tables/NexGen_Cleaned/_committed_3948208819167917375', name='_committed_3948208819167917375', size=113, modificationTime=1739623835000),\n FileInfo(path='dbfs:/FileStore/tables/NexGen_Cleaned/_committed_4891935975649277998', name='_committed_4891935975649277998', size=197, modificationTime=1739640953000),\n FileInfo(path='dbfs:/FileStore/tables/NexGen_Cleaned/_committed_7164279139343481000', name='_committed_7164279139343481000', size=199, modificationTime=1739639938000),\n FileInfo(path='dbfs:/FileStore/tables/NexGen_Cleaned/_committed_vacuum7218355034752194416', name='_committed_vacuum7218355034752194416', size=96, modificationTime=1739639939000),\n FileInfo(path='dbfs:/FileStore/tables/NexGen_Cleaned/_started_4891935975649277998', name='_started_4891935975649277998', size=0, modificationTime=1739640952000),\n FileInfo(path='dbfs:/FileStore/tables/NexGen_Cleaned/_started_7164279139343481000', name='_started_7164279139343481000', size=0, modificationTime=1739639937000),\n FileInfo(path='dbfs:/FileStore/tables/NexGen_Cleaned/part-00000-tid-4891935975649277998-c00da439-5779-46e3-a28c-540c89d3d428-2-1-c000.csv', name='part-00000-tid-4891935975649277998-c00da439-5779-46e3-a28c-540c89d3d428-2-1-c000.csv', size=75981, modificationTime=1739640952000)]"
     ]
    }
   ],
   "source": [
    "\n",
    "# List files in the output directory\n",
    "dbutils.fs.ls(cleaned_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc469a87-f1b4-4955-b506-9279b7113447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download your cleaned CSV from:\nhttps://community.cloud.databricks.com/files/NexGen_Cleaned/\n"
     ]
    }
   ],
   "source": [
    "dbutils.fs.cp(cleaned_output_path, \"dbfs:/FileStore/NexGen_Cleaned\", recurse=True)\n",
    "\n",
    "print(\"Download your cleaned CSV from:\")\n",
    "print(\"https://community.cloud.databricks.com/files/NexGen_Cleaned/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "140df97d-a304-49e3-bf0f-4365a7f741cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned CSV saved at: dbfs:/FileStore/NexGen_Cleaned_Single\n"
     ]
    }
   ],
   "source": [
    "# Save as a SINGLE CSV file\n",
    "df_cleaned.coalesce(1).write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"dbfs:/FileStore/NexGen_Cleaned_Single\")\n",
    "\n",
    "print(\" Cleaned CSV saved at: dbfs:/FileStore/NexGen_Cleaned_Single\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5666dd1e-f9cc-442f-97f8-9e5b47110be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Download your cleaned CSV from:\nhttps://community.cloud.databricks.com/files/final_nexgen_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "files = dbutils.fs.ls(\"dbfs:/FileStore/NexGen_Cleaned_Single\")\n",
    "csv_file = [f.path for f in files if f.name.endswith(\".csv\")][0]\n",
    "\n",
    "# Move to a proper download location\n",
    "dbutils.fs.cp(csv_file, \"dbfs:/FileStore/final_nexgen_cleaned.csv\")\n",
    "\n",
    "print(\" Download your cleaned CSV from:\")\n",
    "print(\"https://community.cloud.databricks.com/files/final_nexgen_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e36ff52b-037c-4278-9e30-c3e3336965d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below \n",
    "Partitioning of the dataset so to have ease in analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d944af5-72b8-4b9e-9fd8-634bcf9e1e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Partitioned CSVs saved at: dbfs:/FileStore/NexGen_Partitioned_CSV\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import shutil\n",
    "\n",
    "# Define Paths\n",
    "partitioned_output_path = \"dbfs:/FileStore/NexGen_Partitioned_CSV\"\n",
    "\n",
    "# Filter Data for 2024 and 2025\n",
    "df_filtered = df_cleaned.filter(col(\"year\").isin([2024, 2025]))\n",
    "\n",
    "# Repartition Data by Location, Year, and Month\n",
    "df_partitioned = df_filtered.repartition(\"location\", \"year\", \"month\")\n",
    "\n",
    "# Save the Partitioned CSVs\n",
    "df_partitioned.write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .partitionBy(\"location\", \"year\", \"month\") \\\n",
    "    .save(partitioned_output_path)\n",
    "\n",
    "print(\" Partitioned CSVs saved at:\", partitioned_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "487c9560-1d63-4b71-83c7-24fa848a8cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_Boston/.csv\n✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_Chicago/.csv\n✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_Dallas/.csv\n✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_Denver/.csv\n✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_Houston/.csv\n✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_Los Angeles/.csv\n✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_Miami/.csv\n✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_New York/.csv\n✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_San Francisco/.csv\n✅ Downloadable CSV saved: dbfs:/FileStore/final_nexgen_Seattle/.csv\n✅ Download Links for Each Location:\nhttps://community.cloud.databricks.com/files/final_nexgen_Boston/.csv\nhttps://community.cloud.databricks.com/files/final_nexgen_Chicago/.csv\nhttps://community.cloud.databricks.com/files/final_nexgen_Dallas/.csv\nhttps://community.cloud.databricks.com/files/final_nexgen_Denver/.csv\nhttps://community.cloud.databricks.com/files/final_nexgen_Houston/.csv\nhttps://community.cloud.databricks.com/files/final_nexgen_Los Angeles/.csv\nhttps://community.cloud.databricks.com/files/final_nexgen_Miami/.csv\nhttps://community.cloud.databricks.com/files/final_nexgen_New York/.csv\nhttps://community.cloud.databricks.com/files/final_nexgen_San Francisco/.csv\nhttps://community.cloud.databricks.com/files/final_nexgen_Seattle/.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to Merge CSVs for Easy Download\n",
    "def merge_csv_files(src_folder, dest_file):\n",
    "    \"\"\"Merges multiple part CSVs into a single file for download.\"\"\"\n",
    "    files = dbutils.fs.ls(src_folder)\n",
    "    csv_files = [f.path for f in files if f.name.endswith(\".csv\")]\n",
    "    \n",
    "    # Combine all part files into one\n",
    "    with open(\"/tmp/temp_combined.csv\", \"wb\") as outfile:\n",
    "        for file in csv_files:\n",
    "            with open(\"/dbfs\" + file[5:], \"rb\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "    \n",
    "    # Move merged file to FileStore for download\n",
    "    dbutils.fs.cp(\"file:/tmp/temp_combined.csv\", dest_file)\n",
    "    print(f\"✅ Downloadable CSV saved: {dest_file}\")\n",
    "\n",
    "# Merge & Move CSVs to FileStore for Download\n",
    "download_links = []\n",
    "locations = [f.name.split(\"=\")[1] for f in dbutils.fs.ls(partitioned_output_path) if \"location=\" in f.name]\n",
    "\n",
    "for location in locations:\n",
    "    src_path = f\"{partitioned_output_path}/location={location}\"\n",
    "    dest_path = f\"dbfs:/FileStore/final_nexgen_{location}.csv\"\n",
    "    \n",
    "    merge_csv_files(src_path, dest_path)\n",
    "    \n",
    "    download_links.append(f\"https://community.cloud.databricks.com/files/final_nexgen_{location}.csv\")\n",
    "\n",
    "print(\"✅ Download Links for Each Location:\")\n",
    "for link in download_links:\n",
    "    print(link)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NexGen_ETL_Pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
